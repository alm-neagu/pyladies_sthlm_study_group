{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 6\n",
    "# Data Loading, Storage, and File Formats\n",
    "- Accessing data is a necessary first step for using most of the tools in this book.\n",
    "- Focus on data input and output using **pandas**.\n",
    "- Reading text files and other more efficient on-disk formats.\n",
    "- Loading data from databases.\n",
    "- Interacting with network sources like web APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Writing Data in Text Format\n",
    "- **pandas** features a number of functions for reading tabular data as a **DataFrame** object.\n",
    "- Most common are **read_csv** and **read_table**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE: Parsing functions in pandas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Function                  | Description |\n",
    "| :---                  |    :----    |\n",
    "|read_csv| Load delimited data from a file, URL, or file-like object; use comma as default delimiter\n",
    "|read_table| Load delimited data from a file, URL, or file-like object; use tab ('\\t') as default delimiter\n",
    "|read_fwf| Read data in fixed-width column format (i.e., no delimiters)\n",
    "|read_clipboard| Version of read_table that reads data from the clipboard; useful for converting tables from web pages\n",
    "|read_excel| Read tabular data from an Excel XLS or XLSX file\n",
    "|read_hdf| Read HDF5 files written by pandas\n",
    "|read_html| Read all tables found in the given HTML document\n",
    "|read_json| Read data from a JSON (JavaScript Object Notation) string representation\n",
    "|read_msgpack| Read pandas data encoded using the MessagePack binary format\n",
    "|read_pickle| Read an arbitrary object stored in Python pickle format\n",
    "|read_sas| Read a SAS dataset stored in one of the SAS system’s custom storage formats\n",
    "|read_sql| Read the results of a SQL query (using SQLAlchemy) as a pandas DataFrame\n",
    "|read_stata| Read a dataset from Stata file format\n",
    "|read_feather| Read the Feather binary file format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional arguments of these function:\n",
    "- **Indexing**: \n",
    "        Can treat one or more columns as the returned DataFrame, and whether to get column names from the file, the user, or not at all.\n",
    "- **Type inference and data conversion**:\n",
    "        This includes the user-defined value conversions and custom list of missing value markers.\n",
    "- **Datetime parsing**:\n",
    "        Includes combining capability, including combining date and time information spread over multiple columns into a single column in the result.\n",
    "- **Iterating**:\n",
    "        Support for iterating over chunks of very large files.\n",
    "- **Unclean data issues**:\n",
    "        Skipping rows or a footer, comments, or other minor things like numeric data with thousands separated by commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because real world data is messy many data loading functions (especially read_csv) have grown very complex in their options over time.\n",
    "- **read_csv** has over 50 parameters\n",
    "- Some of these functions, like **pandas.read_csv**, perform **type inference**, because the column data types are not part of the data format. That means you don’t necessarily have to specify which columns are numeric, integer, boolean, or string.\n",
    "- Other data formats, like HDF5, Feather, and msgpack, have the data types stored in the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.689880Z",
     "start_time": "2021-01-08T17:08:16.328672Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.766112Z",
     "start_time": "2021-01-08T17:08:16.693914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the documetation for read_csv\n",
    "pd.read_csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.797976Z",
     "start_time": "2021-01-08T17:08:16.768055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use read_csv to read ex1.csv into a DataFrame\n",
    "df = pd.read_csv('examples/ex1.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.813902Z",
     "start_time": "2021-01-08T17:08:16.799972Z"
    }
   },
   "outputs": [],
   "source": [
    "# One can specify the delimeter with read_csv\n",
    "pd.read_table('examples/ex1.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.829860Z",
     "start_time": "2021-01-08T17:08:16.815899Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read file without header\n",
    "pd.read_csv('examples/ex2.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.845818Z",
     "start_time": "2021-01-08T17:08:16.830858Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify the columns names\n",
    "pd.read_csv('examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.861798Z",
     "start_time": "2021-01-08T17:08:16.846814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Specify that the 4th column is supposed to be the index of the DataFrame\n",
    "pd.read_csv('examples/ex2.csv',\n",
    "            names=['a', 'b', 'c', 'd', 'message'], index_col=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.893726Z",
     "start_time": "2021-01-08T17:08:16.864791Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a hierarchical index from multiple columns\n",
    "pd.read_csv('examples/csv_mindex.csv',\n",
    "            index_col=['key1', 'key2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.909677Z",
     "start_time": "2021-01-08T17:08:16.895686Z"
    }
   },
   "outputs": [],
   "source": [
    "# Consider a text file that does not have a fixed delimiter\n",
    "# Fields here are separated by a variable amount of whitespace\n",
    "list(open('examples/ex3.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.925606Z",
     "start_time": "2021-01-08T17:08:16.910677Z"
    }
   },
   "outputs": [],
   "source": [
    "# Pass a regular expression as the delimiter\n",
    "pd.read_table('examples/ex3.txt', sep='\\s+')\n",
    "\n",
    "# Because there was one fewer column name than the number of data rows,\n",
    "# read_table infers that the first column should be the DataFrame’s index in this special case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.941591Z",
     "start_time": "2021-01-08T17:08:16.927598Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can skip the first, third, and fourth rows of a file with skiprows\n",
    "list(open('examples/ex4.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.957519Z",
     "start_time": "2021-01-08T17:08:16.943587Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read above file with read_csv\n",
    "pd.read_csv('examples/ex4.csv', skiprows=[0, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling missing values is an important and frequently nuanced part of the file parsing process. \n",
    "- Missing data is usually either not present (empty string) or marked by some sentinel value. \n",
    "- By default, pandas uses a set of commonly occurring sentinels, such as **NA** and **NULL**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.973506Z",
     "start_time": "2021-01-08T17:08:16.959546Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check file with missing values\n",
    "list(open('examples/ex5.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:16.989433Z",
     "start_time": "2021-01-08T17:08:16.975502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read file above using read_csv\n",
    "result = pd.read_csv('examples/ex5.csv')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.005423Z",
     "start_time": "2021-01-08T17:08:16.991426Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "pd.isnull(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE: Some read_csv/read_table function arguments**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument                  | Description |\n",
    "| :---                  |    :----    |\n",
    "|path| String indicating filesystem location, URL, or file-like object\n",
    "|sep or delimiter| Character sequence or regular expression to use to split fields in each row\n",
    "|header| Row number to use as column names; defaults to 0 (first row), but should be None if there is no header row\n",
    "|index_col| Column numbers or names to use as the row index in the result; can be a single name/number or a list of them for a hierarchical index\n",
    "|names| List of column names for result, combine with header=None\n",
    "|skiprows| Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip.\n",
    "|na_values| Sequence of values to replace with NA.\n",
    "|comment| Character(s) to split comments off the end of lines.\n",
    "|parse_dates| Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise can specify a list of column numbers or name to parse. If element of list is tuple or list, will combine multiple columns together and parse to date (e.g., if date/time split across two columns).\n",
    "|keep_date_col| If joining columns to parse date, keep the joined columns; False by default.\n",
    "|converters| Dict containing column number of name mapping to functions (e.g., {'foo': f} would apply the function f to all values in the 'foo' column).\n",
    "|dayfirst| When parsing potentially ambiguous dates, treat as international format (e.g., 7/6/2012 -> June 7, 2012); False by default.\n",
    "|date_parser| Function to use to parse dates.\n",
    "|nrows| Number of rows to read from beginning of file.\n",
    "|iterator| Return a TextParser object for reading file piecemeal.\n",
    "|chunksize| For iteration, size of file chunks.\n",
    "|skip_footer| Number of lines to ignore at end of file.\n",
    "|verbose| Print various parser output information, like the number of missing values placed in non-numeric columns.\n",
    "|encoding| Text encoding for Unicode (e.g., 'utf-8' for UTF-8 encoded text).\n",
    "|squeeze| If the parsed data only contains one column, return a Series.\n",
    "|thousands| Separator for thousands (e.g., ',' or '.')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Text Files in Pieces\n",
    "- When processing very large files or figuring out the right set of arguments to correctly process a large file, you may only want to read in a small piece of a file or iterate through smaller chunks of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.036339Z",
     "start_time": "2021-01-08T17:08:17.006423Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read a large file with read_csv\n",
    "result = pd.read_csv('examples/ex6.csv')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.067258Z",
     "start_time": "2021-01-08T17:08:17.038302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read a small number of rows with nrows\n",
    "pd.read_csv('examples/ex6.csv', nrows=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.083208Z",
     "start_time": "2021-01-08T17:08:17.068255Z"
    }
   },
   "outputs": [],
   "source": [
    "# To read a file in pieces, specify a chunksize as a number of rows\n",
    "chunker = pd.read_csv('examples/ex6.csv', chunksize=1000)\n",
    "chunker\n",
    "\n",
    "# The TextParser object returned by read_csv allows you to iterate over the parts of \n",
    "# the file according to the chunksize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.178959Z",
     "start_time": "2021-01-08T17:08:17.085202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over ex6.csv, aggregating the value counts in the 'key' column\n",
    "tot = pd.Series([], dtype = 'float64')\n",
    "for piece in chunker:\n",
    "    tot = tot.add(piece['key'].value_counts(), fill_value=0)\n",
    "    \n",
    "tot = tot.sort_values(ascending=False)\n",
    "tot[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data to Text Format\n",
    "- Data can also be exported to a delimited format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.210839Z",
     "start_time": "2021-01-08T17:08:17.180922Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read a csv file\n",
    "data = pd.read_csv('examples/ex5.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.226796Z",
     "start_time": "2021-01-08T17:08:17.211838Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write data to a csv file using DataFrame’s to_csv method\n",
    "data.to_csv('examples/output/out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.242754Z",
     "start_time": "2021-01-08T17:08:17.227794Z"
    }
   },
   "outputs": [],
   "source": [
    "# Other delimiters can be used\n",
    "import sys\n",
    "\n",
    "data.to_csv(sys.stdout, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.258711Z",
     "start_time": "2021-01-08T17:08:17.246744Z"
    }
   },
   "outputs": [],
   "source": [
    "# Choose another sentinel value for missing values\n",
    "data.to_csv(sys.stdout, na_rep='NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.274672Z",
     "start_time": "2021-01-08T17:08:17.261705Z"
    }
   },
   "outputs": [],
   "source": [
    "# Disable row and column labels in the output\n",
    "data.to_csv(sys.stdout, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.290653Z",
     "start_time": "2021-01-08T17:08:17.276666Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write only a subset of columns\n",
    "data.to_csv(sys.stdout, index=False, columns=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.306618Z",
     "start_time": "2021-01-08T17:08:17.292656Z"
    }
   },
   "outputs": [],
   "source": [
    "# Series also have a to_csv method\n",
    "dates = pd.date_range('1/1/2000', periods=7)\n",
    "ts = pd.Series(np.arange(7), index=dates)\n",
    "ts.to_csv('examples/output/tseries.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Delimited Formats\n",
    "- Most forms of tabular data from disk using functions like **pandas.read_table**.\n",
    "- However, some manual processing may be necessary in some cases.\n",
    "- For any file with a single-character delimiter, you can use Python’s built-in csv module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.321569Z",
     "start_time": "2021-01-08T17:08:17.307582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example of malformed lines\n",
    "list(open('examples/ex7.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.337501Z",
     "start_time": "2021-01-08T17:08:17.323565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Python’s built-in csv module\n",
    "import csv\n",
    "\n",
    "# You need to open the file before passing it to csv.reader\n",
    "f = open('examples/ex7.csv')\n",
    "reader = csv.reader(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.353482Z",
     "start_time": "2021-01-08T17:08:17.339497Z"
    }
   },
   "outputs": [],
   "source": [
    "# Iterating through the reader like a file yields tuples of values with \n",
    "# any quote characters removed\n",
    "for line in reader:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.369440Z",
     "start_time": "2021-01-08T17:08:17.355492Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, we read the file into a list of lines\n",
    "with open('examples/ex7.csv') as f:\n",
    "    lines = list(csv.reader(f))\n",
    "    \n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.385466Z",
     "start_time": "2021-01-08T17:08:17.371437Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the lines into the header line and the data lines\n",
    "header, values = lines[0], lines[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.401419Z",
     "start_time": "2021-01-08T17:08:17.387424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary of data columns using a dictionary comprehension\n",
    "# and the expression zip(*values), which transposes rows to columns\n",
    "data_dict = {h: v for h, v in zip(header, zip(*values))}\n",
    "data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.417344Z",
     "start_time": "2021-01-08T17:08:17.402384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Construct DataFrame from dict\n",
    "pd.DataFrame.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-06T09:07:26.499041Z",
     "start_time": "2021-01-06T09:07:26.482087Z"
    }
   },
   "source": [
    "## JSON Data\n",
    "- **JSON** (short for JavaScript Object Notation) has become one of the standard formats for sending data by HTTP request between web browsers and other applications.\n",
    "- It is a much more free-form data format than a tabular text form like CSV.\n",
    "- There are several Python libraries for reading and writing JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.433336Z",
     "start_time": "2021-01-08T17:08:17.418341Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a JSON object\n",
    "obj = \"\"\"\n",
    "{\"name\": \"Wes\",\n",
    "\"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\n",
    "\"pet\": null,\n",
    "\"siblings\": [{\"name\": \"Scott\", \"age\": 30, \"pets\": [\"Zeus\", \"Zuko\"]},\n",
    "{\"name\": \"Katie\", \"age\": 38,\n",
    "\"pets\": [\"Sixes\", \"Stache\", \"Cisco\"]}]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.449259Z",
     "start_time": "2021-01-08T17:08:17.434299Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Convert a JSON string with json.loads\n",
    "result = json.loads(obj)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.465216Z",
     "start_time": "2021-01-08T17:08:17.451253Z"
    }
   },
   "outputs": [],
   "source": [
    "# json.dumps converts a Python object back to JSON\n",
    "asjson = json.dumps(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.481177Z",
     "start_time": "2021-01-08T17:08:17.467211Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can pass a list of dicts (which were previously JSON objects) \n",
    "# to the DataFrame constructor and select a subset of the data fields\n",
    "\n",
    "siblings = pd.DataFrame(result['siblings'], columns=['name', 'age', 'pets'])\n",
    "siblings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.513089Z",
     "start_time": "2021-01-08T17:08:17.483173Z"
    }
   },
   "outputs": [],
   "source": [
    "# The pandas.read_json can automatically convert JSON datasets in specific \n",
    "# arrangements into a Series or DataFrame\n",
    "\n",
    "data = pd.read_json('examples/example.json')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.529046Z",
     "start_time": "2021-01-08T17:08:17.514087Z"
    }
   },
   "outputs": [],
   "source": [
    "# To export data from pandas to JSON the to_json methods on Series and DataFrame\n",
    "print(data.to_json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML and HTML: Web Scraping\n",
    "- Python has many libraries for reading and writing data in the ubiquitous HTML and XML formats: **lxml**, **Beautiful Soup**, and **html5lib**.\n",
    "- While **lxml** is comparatively much faster in general, the other libraries can better handle malformed HTML or XML files.\n",
    "- **pandas** has a built-in function, **read_html**, which uses libraries like **lxml** and **Beautiful Soup** to automatically parse tables out of HTML files as DataFrame objects.\n",
    "- The **pandas.read_html** function has a number of options, but by default it searches for and attempts to parse all tabular data contained within table tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.814285Z",
     "start_time": "2021-01-08T17:08:17.531040Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read a html file with pd.read_html function\n",
    "tables = pd.read_html('examples/fdic_failed_bank_list.html')\n",
    "\n",
    "# Check the length of tables and its type\n",
    "print(type(tables), 'length:', len(tables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.830259Z",
     "start_time": "2021-01-08T17:08:17.816278Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select the only element of the list tables\n",
    "failures = tables[0]\n",
    "failures.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing XML with lxml.objectify\n",
    "- **XML** (eXtensible Markup Language) is another common structured data format supporting hierarchical, nested data with metadata.\n",
    "- **XML** and **HTML** are structurally similar, but XML is more general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XML Example**: \n",
    "- The New York Metropolitan Transportation Authority (MTA) publishes a number of data series about its bus and train services.\n",
    "- Here we’ll look at the performance data, which is contained in a set of XML files. \n",
    "- Each train or bus service has a different file (like *Performance_MNR.xml* for the Metro-North Railroad) containing monthly data as a series of XML records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.862155Z",
     "start_time": "2021-01-08T17:08:17.832236Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using lxml.objectify, we parse the file and get a reference to the root node of the\n",
    "# XML file with getroot\n",
    "\n",
    "from lxml import objectify\n",
    "\n",
    "path = 'datasets/mta_perf/Performance_MNR.xml'\n",
    "parsed = objectify.parse(open(path))\n",
    "root = parsed.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.910195Z",
     "start_time": "2021-01-08T17:08:17.864151Z"
    }
   },
   "outputs": [],
   "source": [
    "# root.INDICATOR returns a generator yielding each <INDICATOR> XML element\n",
    "# For each record, we can populate a dict of tag names (like YTD_ACTUAL) to data values\n",
    "# (excluding a few tags)\n",
    "\n",
    "# Define an empty list\n",
    "data = []\n",
    "\n",
    "# Create a list with the tags we want to exclude\n",
    "skip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ',\n",
    "               'DESIRED_CHANGE', 'DECIMAL_PLACES']\n",
    "\n",
    "# Iterate over root.INDICATOR to create a list of dictionaries\n",
    "for elt in root.INDICATOR:\n",
    "    el_data = {}\n",
    "    for child in elt.getchildren():\n",
    "        if child.tag in skip_fields:\n",
    "            continue\n",
    "        el_data[child.tag] = child.pyval\n",
    "    data.append(el_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.941161Z",
     "start_time": "2021-01-08T17:08:17.912180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert this list of dicts into a DataFrame\n",
    "perf = pd.DataFrame(data)\n",
    "perf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Data Formats\n",
    "- One of the easiest ways to store data (also known as serialization) efficiently is in **binary format**.\n",
    "- Python has the built-in **pickle** serialization.\n",
    "- pandas objects all have a **to_pickle** method that writes the data to disk in pickle format.\n",
    "- **pickle** is only recommended as a short-term storage format. The problem is that it is hard to guarantee that the format will be stable over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.957093Z",
     "start_time": "2021-01-08T17:08:17.943134Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data from csv file\n",
    "frame = pd.read_csv('examples/ex1.csv')\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.973052Z",
     "start_time": "2021-01-08T17:08:17.959090Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save data into binary format\n",
    "frame.to_pickle('examples/output/frame_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:17.989012Z",
     "start_time": "2021-01-08T17:08:17.975049Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read a “pickled” object stored in a file with pandas.read_pickle\n",
    "pd.read_pickle('examples/output/frame_pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **pandas** has built-in support for two more binary data formats: **HDF5** and **Message‐Pack**.\n",
    "- And for **bcolz** =  A compressable column-oriented binary format based on the Blosc compression library.\n",
    "- And for **Feather** = A cross-language column-oriented file format from the R programming community. Feather uses the Apache Arrow columnar memory format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using HDF5 Format\n",
    "- **HDF** in **HDF5** stands for hierarchical data format.\n",
    "- It is available as a C library, and it has interfaces available in many other languages, including Java, Julia, MATLAB, and Python.\n",
    "- Each **HDF5** file can store multiple datasets and supporting metadata.\n",
    "- Compared with simpler formats, **HDF5** supports on-the-fly compression with a variety of compression modes, enabling data with repeated patterns to be stored more efficiently.\n",
    "- HDF5 can be a good choice for working with very large datasets that don’t fit into memory, as you can efficiently read and write small sections of much larger arrays.\n",
    "- While it’s possible to directly access **HDF5** files using either the **PyTables** or **h5py** libraries, **pandas** provides a high-level interface that simplifies storing Series and\n",
    "DataFrame object. \n",
    "- The **HDFStore** class works like a dict and handles the low-level details.\n",
    "- If you are processing data that is stored on remote servers, likeAmazon S3 or HDFS, using a different binary format designed for distributed storage like **Apache Parquet** may be more suitable.\n",
    "- HDF5 is not a database. It is best suited for write-once, read-many datasets. While data can be added to a file at any time, if multiple writers do so simultaneously, the file can become corrupted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:18.003971Z",
     "start_time": "2021-01-08T17:08:17.990007Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "frame = pd.DataFrame({'a': np.random.randn(100)})\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:08:18.510838Z",
     "start_time": "2021-01-08T17:08:18.005967Z"
    },
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# Create an HDF5 file\n",
    "store = pd.HDFStore('mydata.h5')\n",
    "\n",
    "# Store the frame DataFrame in the HDF5 file \n",
    "store['obj1'] = frame\n",
    "\n",
    "# Store a Series in the HDF5 file \n",
    "store['obj1_col'] = frame['a']\n",
    "\n",
    "# Store the failures DataFrame in the HDF5 file\n",
    "store['obj2'] = failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:10:14.503182Z",
     "start_time": "2021-01-08T17:10:14.496193Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print details about the HDF5 file we created\n",
    "print(store.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:11:16.533560Z",
     "start_time": "2021-01-08T17:11:16.499653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Objects contained in the HDF5 file can then be retrieved with the same dict-like API\n",
    "store['obj2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:19:46.752597Z",
     "start_time": "2021-01-08T17:19:46.728670Z"
    }
   },
   "outputs": [],
   "source": [
    "# HDFStore supports two storage schemas, 'fixed' and 'table' \n",
    "# 'table'  is generally slower, but it supports query operations using a special syntax\n",
    "\n",
    "store.put('obj2', frame, format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:20:28.852970Z",
     "start_time": "2021-01-08T17:20:28.826043Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select rows from obj2 stored in 'mydata.h5'\n",
    "store.select('obj2', where=['index >= 10 and index <= 15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:21:31.471513Z",
     "start_time": "2021-01-08T17:21:31.459582Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only when you run the close() method the data is written to disk\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:23:52.440173Z",
     "start_time": "2021-01-08T17:23:52.407291Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandas.read_hdf function gives you a shortcut\n",
    "pd.read_hdf('mydata.h5', 'obj2', where=['index < 5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Microsoft Excel Files\n",
    "- **pandas** also supports reading tabular data stored in Excel 2003 (and higher) files using either the ExcelFile class or **pandas.read_excel** function. \n",
    "- Internally these tools use the add-on packages **xlrd** and **openpyxl** to read XLS and XLSX files, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:33:32.348898Z",
     "start_time": "2021-01-08T17:33:32.295041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an instance by passing a path to an xls or xlsx file\n",
    "xlsx = pd.ExcelFile('examples/ex1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:34:33.188980Z",
     "start_time": "2021-01-08T17:34:33.177974Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data stored in a sheet can then be read into DataFrame with parse\n",
    "pd.read_excel(xlsx, 'Sheet1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:35:39.169976Z",
     "start_time": "2021-01-08T17:35:39.148050Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can also simply pass the filename to pandas.read_excel\n",
    "# But if you are reading multiple sheets in a file, then it is faster to create the ExcelFile\n",
    "\n",
    "frame = pd.read_excel('examples/ex1.xlsx', 'Sheet1')\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:37:34.605462Z",
     "start_time": "2021-01-08T17:37:34.457961Z"
    }
   },
   "outputs": [],
   "source": [
    "# To write pandas data to Excel format, you must first create an ExcelWriter, \n",
    "# then write data to it using pandas objects’ to_excel method\n",
    "\n",
    "writer = pd.ExcelWriter('examples/output/ex2.xlsx')\n",
    "frame.to_excel(writer, 'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:38:05.154327Z",
     "start_time": "2021-01-08T17:38:05.100163Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can also pass a file path to to_excel and avoid the ExcelWriter\n",
    "frame.to_excel('examples/output/ex2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Web APIs\n",
    "- Many websites have **public APIs** providing data feeds via JSON or some other format.\n",
    "- An easy way to access these APIs from Python is via the requests package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE:** - find the last 30 GitHub issues for pandas on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:42:05.343768Z",
     "start_time": "2021-01-08T17:42:05.264321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:43:46.638066Z",
     "start_time": "2021-01-08T17:43:45.836748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the url from where to get the data\n",
    "url = 'https://api.github.com/repos/pandas-dev/pandas/issues'\n",
    "\n",
    "# Get the data via requests.get method\n",
    "resp = requests.get(url)\n",
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:44:49.092391Z",
     "start_time": "2021-01-08T17:44:49.073407Z"
    }
   },
   "outputs": [],
   "source": [
    "# The Response object’s json method will return a dictionary containing JSON parsed\n",
    "# into native Python objects\n",
    "\n",
    "data = resp.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:46:12.534490Z",
     "start_time": "2021-01-08T17:46:12.526480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Each element in data is a dictionary containing all of the data found on a GitHub\n",
    "# issue page (except for the comments)\n",
    "\n",
    "# Get the title for the first issue\n",
    "data[0]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:46:49.683610Z",
     "start_time": "2021-01-08T17:46:49.630786Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can pass data directly to DataFrame and extract fields of interest\n",
    "issues = pd.DataFrame(data, columns=['number', 'title',\n",
    "                                     'labels', 'state'])\n",
    "\n",
    "issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with Databases\n",
    "- In a business setting, most data may not be stored in text or Excel files. \n",
    "- **SQL-based** relational databases (such as SQL Server, PostgreSQL, and MySQL) are in wide use.\n",
    "- Loading data from SQL into a DataFrame is fairly straightforward, and pandas has some functions to simplify the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE:** - create a SQLite database using Python’s built-in sqlite3 driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T17:50:17.261832Z",
     "start_time": "2021-01-08T17:50:17.256847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import neccessary libraries\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:21:10.575723Z",
     "start_time": "2021-01-08T18:21:10.568743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty SQL table\n",
    "query = \"\"\"\n",
    "    CREATE TABLE test\n",
    "    (a VARCHAR(20), b VARCHAR(20),\n",
    "     c REAL, d INTEGER\n",
    "    );\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:21:17.617926Z",
     "start_time": "2021-01-08T18:21:17.557524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the connection to the database\n",
    "con = sqlite3.connect('mydata.sqlite')\n",
    "\n",
    "# Execute the query\n",
    "con.execute(query)\n",
    "\n",
    "# Commit the changes to the database\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:23:09.005543Z",
     "start_time": "2021-01-08T18:23:08.985630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Insert a few rows of data into test table\n",
    "data = [('Atlanta', 'Georgia', 1.25, 6),\n",
    "        ('Tallahassee', 'Florida', 2.6, 3),\n",
    "        ('Sacramento', 'California', 1.7, 5)]\n",
    "\n",
    "stmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\n",
    "\n",
    "con.executemany(stmt, data)\n",
    "\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:23:11.635323Z",
     "start_time": "2021-01-08T18:23:11.617328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Most Python SQL drivers (PyODBC, psycopg2, MySQLdb, pymssql, etc.) return a list\n",
    "# of tuples when selecting data from a table\n",
    "cursor = con.execute('select * from test')\n",
    "\n",
    "# Fetch all rows\n",
    "rows = cursor.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:23:13.578237Z",
     "start_time": "2021-01-08T18:23:13.561280Z"
    }
   },
   "outputs": [],
   "source": [
    "# Column names are contained in the cursor’s description attribute\n",
    "cursor.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:23:16.359605Z",
     "start_time": "2021-01-08T18:23:16.348668Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can pass the list of tuples to the DataFrame constructor + the columns names\n",
    "pd.DataFrame(rows, columns=[x[0] for x in cursor.description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQLAlchemy\n",
    "- The **SQLAlchemy** project is a popular Python SQL toolkit that abstracts away many of the common differences between SQL databases. \n",
    "- **pandas** has a **read_sql** function that enables you to read data easily from a general SQLAlchemy connection.\n",
    "- Here, we’ll connect to the same SQLite database with SQLAlchemy and read data from the table created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:21:59.035309Z",
     "start_time": "2021-01-08T18:21:58.881026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the package\n",
    "import sqlalchemy as sqla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:25:58.114891Z",
     "start_time": "2021-01-08T18:25:58.105580Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create teh sqla engine\n",
    "db = sqla.create_engine('sqlite:///mydata.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-08T18:26:43.086214Z",
     "start_time": "2021-01-08T18:26:43.054303Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use pandas read_sql function to read the data directly into a DataFrame\n",
    "pd.read_sql('select * from test', db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "100px",
    "left": "146px",
    "top": "145px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
