{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 7\n",
    "# Data Cleaning and Preparation\n",
    "- Data preparation (loading, cleaning, transforming, rearranging) is reported to take up to 80% or more of an analyst's time.\n",
    "- **pandas** along with buil-in Python features provide a high-level, flexible, and fast set of tools to manipulate data into the right form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "- Missing data occurs in many data analysis applications.\n",
    "- **pandas** try to make working with missing data as painless as possible.\n",
    "- For example all of the descriptive statistics on **pandas objects** exclude missing data by default.\n",
    "- For numeric data, pandas uses the floating-point value **NaN (Not a Number)** to represent missing data - this is called a *sentinel value*.\n",
    "- For other data types pandas uses **NA (not available)** to represent missing values. \n",
    "- In statistics applications, **NA data** may either be data that does not exist or that exists but was not observed (through problems with data collection, for example). \n",
    "- When cleaning up data for analysis, it is often important to do analysis on the missing data itself to identify data collection problems or potential biases in the data caused by missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.383029Z",
     "start_time": "2021-01-27T16:21:39.864599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.399027Z",
     "start_time": "2021-01-27T16:21:40.384989Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a pandas Series of strings contaning one missing value\n",
    "string_data = pd.Series(['aardvark', 'artichoke', np.nan, 'avocado'])\n",
    "string_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.414763Z",
     "start_time": "2021-01-27T16:21:40.400249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use isnull function to check for missing values\n",
    "string_data.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.430201Z",
     "start_time": "2021-01-27T16:21:40.415764Z"
    }
   },
   "outputs": [],
   "source": [
    "# The built-in Python None value is also treated as NA in object arrays\n",
    "\n",
    "# Replace the first element in string_data with None\n",
    "string_data[0] = None\n",
    "\n",
    "# Check for missing values\n",
    "string_data.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE**: NA handling methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument                  | Description |\n",
    "| :---                  |    :----    |\n",
    "|dropna| Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much missing data to tolerate.\n",
    "|fillna| Fill in missing data with some value or using an interpolation method such as 'ffill' or 'bfill'.\n",
    "|isnull| Return boolean values indicating which values are missing/NA.\n",
    "|notnull| Negation of isnull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Out Missing Data\n",
    "- You can filter out missing data by hand using **pandas.isnull** and boolean indexing.\n",
    "- Or you can use the **dropna** function. \n",
    "- On a Series, **dropna** returns the Series with only the non-null data and index values.\n",
    "- With DataFrame objects, things are a bit more complex. You may want to drop rows or columns that are all NA or only those containing any NAs. **dropna** by default drops any row containing a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.446179Z",
     "start_time": "2021-01-27T16:21:40.432196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Series that contains missing values\n",
    "data = pd.Series([1, np.nan, 3.5, np.nan, 7])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.462237Z",
     "start_time": "2021-01-27T16:21:40.447515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use dropna to remove the missing values\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.478334Z",
     "start_time": "2021-01-27T16:21:40.463238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with missing values\n",
    "data = pd.DataFrame([[1., 6.5, 3.], [1., np.nan, np.nan],\n",
    "                     [np.nan, np.nan, np.nan], [np.nan, 6.5, 3.]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.508913Z",
     "start_time": "2021-01-27T16:21:40.480289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use dropna to remove row with missing values\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.524906Z",
     "start_time": "2021-01-27T16:21:40.511906Z"
    }
   },
   "outputs": [],
   "source": [
    "# Passing how='all' will only drop rows that are all NA\n",
    "data.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.540896Z",
     "start_time": "2021-01-27T16:21:40.526011Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a 4th column with all values NA\n",
    "data[4] = np.nan\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.556912Z",
     "start_time": "2021-01-27T16:21:40.541854Z"
    }
   },
   "outputs": [],
   "source": [
    "# To drop columns in the same way, pass axis=1\n",
    "data.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Suppose you want to keep only rows containing a certain number of observations. \n",
    "- You can indicate this with the **thresh** argument for **dropna**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.572867Z",
     "start_time": "2021-01-27T16:21:40.557924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with 7 rows and 3 columns\n",
    "df = pd.DataFrame(np.random.randn(7, 3))\n",
    "\n",
    "# Replace first 4 values for column 1 with NA values\n",
    "df.iloc[:4, 1] = np.nan\n",
    "\n",
    "# Replace the first 2 values for column 2 with NA values\n",
    "df.iloc[:2, 2] = np.nan\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.588875Z",
     "start_time": "2021-01-27T16:21:40.573877Z"
    }
   },
   "outputs": [],
   "source": [
    "# If we use dropnan with no arguments all rows with at least 1 NA value will be filtered\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.604306Z",
     "start_time": "2021-01-27T16:21:40.589874Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using thresh argument we can indicate how many NA values in a row are allowed\n",
    "df.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling In Missing Data\n",
    "- Rather than filtering out missing data you may want to fill in the “holes” in any number of ways. \n",
    "- For most purposes, the **fillna** method is the workhorse function to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.619992Z",
     "start_time": "2021-01-27T16:21:40.605316Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calling fillna with a constant replaces missing values with that value\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.635752Z",
     "start_time": "2021-01-27T16:21:40.620953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calling fillna with a dict, you can use a different fill value for each column\n",
    "df.fillna({1: 0.5, 2: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.651426Z",
     "start_time": "2021-01-27T16:21:40.637751Z"
    }
   },
   "outputs": [],
   "source": [
    "# fillna returns a new object, but you can modify the existing object in-place\n",
    "df.fillna(0, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.667268Z",
     "start_time": "2021-01-27T16:21:40.652392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new DataFrame\n",
    "df2 = pd.DataFrame(np.random.randn(6, 3))\n",
    "\n",
    "# Insert some missing values\n",
    "df2.iloc[2:, 1] = np.nan\n",
    "df2.iloc[4:, 2] = np.nan\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.682856Z",
     "start_time": "2021-01-27T16:21:40.668228Z"
    }
   },
   "outputs": [],
   "source": [
    "# The same interpolation methods available for reindexing can be used with fillna\n",
    "df2.fillna(method='ffill')\n",
    "\n",
    "# 'ffill' = forward fill method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.698323Z",
     "start_time": "2021-01-27T16:21:40.684818Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new Series with missing values\n",
    "data = pd.Series([1., np.nan, 3.5, np.nan, 7])\n",
    "\n",
    "# For example you might pass the mean or median value of a Series\n",
    "data.fillna(data.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE**: *fillna* function arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument                  | Description |\n",
    "| :---                  |    :----    |\n",
    "|value| Scalar value or dict-like object to use to fill missing values\n",
    "|method| Interpolation; by default 'ffill' if function called with no other arguments\n",
    "|axis| Axis to fill on; default axis=0\n",
    "|inplace| Modify the calling object without producing a copy\n",
    "|limit| For forward and backward filling, maximum number of consecutive periods to fill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.713972Z",
     "start_time": "2021-01-27T16:21:40.699323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame containing duplicates\n",
    "df3 = pd.DataFrame({'k1': ['one', 'two'] * 3 + ['two'],\n",
    "                    'k2': [1, 1, 2, 3, 3, 4, 4]})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.729871Z",
     "start_time": "2021-01-27T16:21:40.714974Z"
    }
   },
   "outputs": [],
   "source": [
    "# The DataFrame method duplicated returns a boolean Series indicating whether each\n",
    "# row is a duplicate (has been observed in a previous row) or not\n",
    "\n",
    "df3.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.745752Z",
     "start_time": "2021-01-27T16:21:40.732869Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop_duplicates returns a DataFrame where the duplicated array is False\n",
    "df3.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.761225Z",
     "start_time": "2021-01-27T16:21:40.748708Z"
    }
   },
   "outputs": [],
   "source": [
    "# Filter duplicates only based on the 'k1' column\n",
    "df3.drop_duplicates(['k1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.777307Z",
     "start_time": "2021-01-27T16:21:40.764050Z"
    }
   },
   "outputs": [],
   "source": [
    "# duplicated and drop_duplicates by default keep the first observed value combination\n",
    "# Passing keep='last' will return the last one\n",
    "\n",
    "df3.drop_duplicates(['k1', 'k2'], keep='last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Data Using a Function or Mapping\n",
    "- For many datasets, you may wish to perform some transformation based on the values in an array, Series, or column in a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.792822Z",
     "start_time": "2021-01-27T16:21:40.778995Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with data about various kinds of meats\n",
    "data = pd.DataFrame({'food': ['bacon', 'pulled pork', 'bacon',\n",
    "                              'Pastrami', 'corned beef', 'Bacon',\n",
    "                              'pastrami', 'honey ham', 'nova lox'],\n",
    "                     'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.808571Z",
     "start_time": "2021-01-27T16:21:40.794486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose you wanted to add a column indicating the type of animal that each food came from\n",
    "\n",
    "# Create a dict to map each meat to the animal\n",
    "meat_to_animal = {\n",
    "'bacon': 'pig',\n",
    "'pulled pork': 'pig',\n",
    "'pastrami': 'cow',\n",
    "'corned beef': 'cow',\n",
    "'honey ham': 'pig',\n",
    "'nova lox': 'salmon'\n",
    "}\n",
    "\n",
    "# The map method on a Series accepts a function or dict-like object containing a mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.824525Z",
     "start_time": "2021-01-27T16:21:40.809535Z"
    }
   },
   "outputs": [],
   "source": [
    "# First we need to convert each value from 'food' column to lowercase using the str.lower\n",
    "lowercased = data['food'].str.lower()\n",
    "lowercased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.840530Z",
     "start_time": "2021-01-27T16:21:40.825488Z"
    }
   },
   "outputs": [],
   "source": [
    "# Nowe we can use the Series map method to create an extra column called 'animal'\n",
    "data['animal'] = lowercased.map(meat_to_animal)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.855939Z",
     "start_time": "2021-01-27T16:21:40.841490Z"
    }
   },
   "outputs": [],
   "source": [
    "# Doing the same thing using a lambda function\n",
    "data['food'].map(lambda x: meat_to_animal[x.lower()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Values\n",
    "- Filling in missing data with the **fillna** method is a special case of more general value replacement. \n",
    "- **map** can be used to modify a subset of values in an object but **replace** provides a simpler and more flexible way to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.871785Z",
     "start_time": "2021-01-27T16:21:40.856945Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Series\n",
    "data = pd.Series([1., -999., 2., -999., -1000., 3.])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.887652Z",
     "start_time": "2021-01-27T16:21:40.873572Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can use replace to modify certail values\n",
    "data.replace(-999, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.903152Z",
     "start_time": "2021-01-27T16:21:40.889657Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace multiple values at once\n",
    "data.replace([-999, -1000], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.918394Z",
     "start_time": "2021-01-27T16:21:40.904111Z"
    }
   },
   "outputs": [],
   "source": [
    "# To use a different replacement for each value, pass a list of substitutes\n",
    "data.replace([-999, -1000], [np.nan, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Axis Indexes\n",
    "- Like values in a Series, axis labels can be similarly transformed by a function or mapping of some form to produce new, differently labeled objects. \n",
    "- You can also modify the axes in-place without creating a new data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.933898Z",
     "start_time": "2021-01-27T16:21:40.919903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "data = pd.DataFrame(np.arange(12).reshape((3, 4)),\n",
    "                    index=['Ohio', 'Colorado', 'New York'],\n",
    "                    columns=['one', 'two', 'three', 'four'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.949898Z",
     "start_time": "2021-01-27T16:21:40.934898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Like a Series, the axis indexes have a map method\n",
    "data.index = data.index.map(lambda x: x[:4].upper())\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.965559Z",
     "start_time": "2021-01-27T16:21:40.950418Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you want to create a transformed version of a dataset without modifying the original, \n",
    "# a useful method is rename\n",
    "data.rename(index=str.title, columns=str.upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretization and Binning\n",
    "- Continuous data is often discretized or otherwise separated into “bins” for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.980967Z",
     "start_time": "2021-01-27T16:21:40.967559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppose you have data about a group of people and you want to group\n",
    "# them into discrete age buckets\n",
    "\n",
    "ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:40.996400Z",
     "start_time": "2021-01-27T16:21:40.982967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let’s divide these into bins of 18 to 25, 26 to 35, 36 to 60, and finally 61 and older\n",
    "bins = [18, 25, 35, 60, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.012123Z",
     "start_time": "2021-01-27T16:21:40.998400Z"
    }
   },
   "outputs": [],
   "source": [
    "# To create the actual bins for the data we can use the pandas.cut function\n",
    "cats = pd.cut(ages, bins)\n",
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pandas.cut**:\n",
    "- The object pandas returns is a special **Categorical object**. \n",
    "- The output describes the bins computed by **pandas.cut** \n",
    "- It contains a categories array specifying the distinct **category names** along with a labeling for the ages data in the **codes attribute**.\n",
    "- **pd.value_counts(cats)** are the bin counts for the result of **pandas.cut**\n",
    "- You can change which side is closed by passing **right=False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.028047Z",
     "start_time": "2021-01-27T16:21:41.013123Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the codes\n",
    "cats.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.043333Z",
     "start_time": "2021-01-27T16:21:41.030063Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the categories\n",
    "cats.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.059308Z",
     "start_time": "2021-01-27T16:21:41.044299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the bin count\n",
    "pd.value_counts(cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.075168Z",
     "start_time": "2021-01-27T16:21:41.060317Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can also pass your own bin names by passing a list or array to the labels option\n",
    "group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']\n",
    "pd.cut(ages, bins, labels=group_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.090970Z",
     "start_time": "2021-01-27T16:21:41.076162Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you pass an integer number of bins to cut instead of explicit bin edges, it will compute \n",
    "# equal-length bins based on the minimum and maximum values in the data\n",
    "\n",
    "data = np.random.randint(20, size=20)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.107024Z",
     "start_time": "2021-01-27T16:21:41.091970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create 4 bins of equal-length\n",
    "cats = pd.cut(data, 4, precision=0)\n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.123024Z",
     "start_time": "2021-01-27T16:21:41.108024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of values in each bin\n",
    "pd.value_counts(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A closely related function, **qcut**, bins the data based on sample **quantiles**. \n",
    "- Depending on the distribution of the data, using cut will not usually result in each bin having the same number of data points. \n",
    "- Since qcut uses sample quantiles instead, by definition you will obtain roughly equal-size bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.138412Z",
     "start_time": "2021-01-27T16:21:41.125024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a sample of normally distributed numbers\n",
    "data = np.random.randn(1000)\n",
    "\n",
    "# Cut into quartiles\n",
    "cats = pd.qcut(data, 4) \n",
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.154370Z",
     "start_time": "2021-01-27T16:21:41.139365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Count the number of values in each bin\n",
    "pd.value_counts(cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting and Filtering Outliers\n",
    "- Filtering or transforming outliers is largely a matter of applying array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.185074Z",
     "start_time": "2021-01-27T16:21:41.155365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Consider a DataFrame with some normally distributed data\n",
    "data = pd.DataFrame(np.random.randn(1000, 4))\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.200508Z",
     "start_time": "2021-01-27T16:21:41.187074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find values in one of the columns exceeding 3 in absolute value\n",
    "col = data[2]\n",
    "col[np.abs(col) > 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.215945Z",
     "start_time": "2021-01-27T16:21:41.201508Z"
    }
   },
   "outputs": [],
   "source": [
    "# To select all rows having a value exceeding 3 or –3, you can use the any method on a\n",
    "# boolean DataFrame\n",
    "\n",
    "data[(np.abs(data) > 3).any(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation and Random Sampling\n",
    "- Permuting (randomly reordering) a Series or the rows in a DataFrame is easy to do using the **numpy.random.permutation** function. \n",
    "- Calling **permutation** with the length of the axis you want to permute produces an array of integers indicating the new ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.232059Z",
     "start_time": "2021-01-27T16:21:41.217945Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame(np.arange(5 * 4).reshape((5, 4)))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.247688Z",
     "start_time": "2021-01-27T16:21:41.233059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the permutation function to create a sampler array\n",
    "sampler = np.random.permutation(5)\n",
    "sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.263436Z",
     "start_time": "2021-01-27T16:21:41.248688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the sampler array as input for take function = Return the elements in the given \n",
    "# positional indices along an axis\n",
    "df.take(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.279347Z",
     "start_time": "2021-01-27T16:21:41.266436Z"
    }
   },
   "outputs": [],
   "source": [
    "# To select a random subset without replacement, you can use the sample method\n",
    "df.sample(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Indicator/Dummy Variables\n",
    "- Another type of transformation for statistical modeling or machine learning applications is converting a categorical variable into a **dummy** or **indicator matrix**. \n",
    "- If a column in a DataFrame has k distinct values, you would derive a matrix or DataFrame with k columns containing all 1s and 0s. \n",
    "- pandas has a **get_dummies** function for doing this, though devising one yourself is not difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.294609Z",
     "start_time": "2021-01-27T16:21:41.281094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\n",
    "                   'data1': range(6)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.310240Z",
     "start_time": "2021-01-27T16:21:41.295630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use get_dummies on key column function to derive a matrix with 3 columns\n",
    "pd.get_dummies(df['key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.326217Z",
     "start_time": "2021-01-27T16:21:41.311234Z"
    }
   },
   "outputs": [],
   "source": [
    "# get_dummies has a prefix argument if you want to add a prefix to the columns in the \n",
    "# indicator DataFrame\n",
    "\n",
    "# Create the dummy matrix with prefix\n",
    "dummies = pd.get_dummies(df['key'], prefix='key')\n",
    "\n",
    "# Join the dummy matrix with the original data\n",
    "df_with_dummy = df[['data1']].join(dummies)\n",
    "df_with_dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXAMPLE**: MovieLens 1M dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.357937Z",
     "start_time": "2021-01-27T16:21:41.327940Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define name for the columns\n",
    "mnames = ['movie_id', 'title', 'genres']\n",
    "\n",
    "# Read the data from movies.dat file\n",
    "movies = pd.read_table('datasets/movielens/movies.dat', sep='::',\n",
    "                       header=None, names=mnames)\n",
    "\n",
    "movies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.373452Z",
     "start_time": "2021-01-27T16:21:41.358903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since a row in the DataFrame belongs to multiple categories\n",
    "# Some wrangling is necessary to get the dummy matrix\n",
    "\n",
    "# First, we extract the list of unique genres in the dataset\n",
    "all_genres = []\n",
    "\n",
    "for x in movies.genres:\n",
    "    all_genres.extend(x.split('|'))\n",
    "\n",
    "genres = pd.unique(all_genres)\n",
    "\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.388262Z",
     "start_time": "2021-01-27T16:21:41.375454Z"
    }
   },
   "outputs": [],
   "source": [
    "# We start with a DataFrame of all zeros\n",
    "zero_matrix = np.zeros((len(movies), len(genres)))\n",
    "zero_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.419224Z",
     "start_time": "2021-01-27T16:21:41.389237Z"
    }
   },
   "outputs": [],
   "source": [
    "# Rename the columns using the list of unq genres we created before\n",
    "dummies = pd.DataFrame(zero_matrix, columns=genres)\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.435264Z",
     "start_time": "2021-01-27T16:21:41.420241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select first element as an example\n",
    "gen = movies.genres[0]\n",
    "\n",
    "# Split the text based on '|'\n",
    "gen.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.451259Z",
     "start_time": "2021-01-27T16:21:41.436264Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the dummies.columns to compute the column indices for each genre\n",
    "dummies.columns.get_indexer(gen.split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.928659Z",
     "start_time": "2021-01-27T16:21:41.453225Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can use .iloc to set values based on these indices:\n",
    "for i, gen in enumerate(movies.genres):\n",
    "    indices = dummies.columns.get_indexer(gen.split('|'))\n",
    "    dummies.iloc[i, indices] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.944383Z",
     "start_time": "2021-01-27T16:21:41.929660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combine this with movies\n",
    "movies_windic = movies.join(dummies.add_prefix('Genre_'))\n",
    "movies_windic.iloc[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation\n",
    "### String Object Methods\n",
    "- In many string munging and scripting applications, built-in string methods are sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.974898Z",
     "start_time": "2021-01-27T16:21:41.969379Z"
    }
   },
   "outputs": [],
   "source": [
    "# As an example, a comma-separated string can be broken into pieces with split\n",
    "val = 'a,b, guido'\n",
    "val.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:41.990954Z",
     "start_time": "2021-01-27T16:21:41.977892Z"
    }
   },
   "outputs": [],
   "source": [
    "# split is often combined with strip to trim whitespace (including line breaks)\n",
    "pieces = [x.strip() for x in val.split(',')]\n",
    "pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.006923Z",
     "start_time": "2021-01-27T16:21:41.992954Z"
    }
   },
   "outputs": [],
   "source": [
    "# These substrings could be concatenated together with a two-colon delimiter using addition\n",
    "first, second, third = pieces\n",
    "first + '::' + second + '::' + third"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.022936Z",
     "start_time": "2021-01-27T16:21:42.007928Z"
    }
   },
   "outputs": [],
   "source": [
    "# But this isn’t a practical generic method. A faster and more Pythonic way is to pass a\n",
    "# list or tuple to the join method on the string '::'\n",
    "'::'.join(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.038744Z",
     "start_time": "2021-01-27T16:21:42.024024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using Python’s in keyword is the best way to detect a substring\n",
    "'guido' in val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.054549Z",
     "start_time": "2021-01-27T16:21:42.039715Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the index of a certain substring\n",
    "val.index(',')\n",
    "\n",
    "# index raises an exception if the substring isn’t found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.069823Z",
     "start_time": "2021-01-27T16:21:42.055543Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can also use find to get the index of asub string\n",
    "val.find('a')\n",
    "\n",
    "# find return -1 if a substring isn't found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.085184Z",
     "start_time": "2021-01-27T16:21:42.070555Z"
    }
   },
   "outputs": [],
   "source": [
    "# count returns the number of occurrences of a particular substring\n",
    "val.count(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.101146Z",
     "start_time": "2021-01-27T16:21:42.086206Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace will substitute occurrences of one pattern for another\n",
    "# It is commonly used to delete patterns, too, by passing an empty string\n",
    "val.replace(',', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE**: Python built-in string methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument                  | Description |\n",
    "| :---                  |    :----    |\n",
    "|count| Return the number of non-overlapping occurrences of substring in the string.\n",
    "|endswith| Returns True if string ends with suffix.\n",
    "|startswith| Returns True if string starts with prefix.\n",
    "|join| Use string as delimiter for concatenating a sequence of other strings.\n",
    "|index |Return position of first character in substring if found in the string; raises ValueError if not found.\n",
    "|find| Return position of first character of rst occurrence of substring in the string; like index, but returns –1 if not found.\n",
    "|rfind| Return position of first character of last occurrence of substring in the string; returns –1 if not found.\n",
    "|replace| Replace occurrences of string with another string.\n",
    "|strip, rstrip, lstrip| Trim whitespace, including newlines; equivalent to x.strip() (and rstrip, lstrip, respectively) for each element.\n",
    "|split| Break string into list of substrings using passed delimiter.\n",
    "|lower| Convert alphabet characters to lowercase.\n",
    "|upper| Convert alphabet characters to uppercase.\n",
    "|casefold| Convert characters to lowercase, and convert any region-specific variable character  ombinations to a common comparable form.\n",
    "|ljust, rjust| Left justify or right justify, respectively; pad opposite side of string with spaces (or some other fill character) to return a string with a minimum width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "- Regular expressions provide a flexible way to search or match (often more complex) string patterns in text. \n",
    "- A single expression, commonly called a **regex**, is a string formed according to the regular expression language. \n",
    "- Python’s built-in **re module** is responsible for applying regular expressions to strings.\n",
    "- The **re module** functions fall into three categories: pattern matching, substitution, and splitting. \n",
    "- A **regex** describes a pattern to locate in the text, which can then be used for many purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.116575Z",
     "start_time": "2021-01-27T16:21:42.102658Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the re module\n",
    "import re\n",
    "\n",
    "# Define a string with variable number of whitespace characters (tabs, spaces, and newlines)\n",
    "text = \"foo bar\\t baz \\tqux\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.131691Z",
     "start_time": "2021-01-27T16:21:42.117651Z"
    }
   },
   "outputs": [],
   "source": [
    "# To remvoe the white spaces you can use the regex describing one or more whitespace \n",
    "# characters \\s+ combined with re.split method\n",
    "re.split('\\s+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.147080Z",
     "start_time": "2021-01-27T16:21:42.132651Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you want to get a list of all patterns matching the regex, you can use the findall method\n",
    "re.findall('\\s+', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile Regex**:\n",
    "- When you call **re.split('\\s+', text)**, the regular expression is first compiled, and then its split method is called on the passed text. \n",
    "- You can compile the regex yourself with **re.compile**, forming a reusable regex object.\n",
    "- Creating a regex object with re.compile is highly recommended if you intend to apply the same expression to many strings; doing so will save CPU cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.162351Z",
     "start_time": "2021-01-27T16:21:42.148081Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let’s consider a block of text and a regular expression capable of identifying \n",
    "# most email addresses\n",
    "\n",
    "text = \"\"\"Dave dave@google.com\n",
    "Steve steve@gmail.com\n",
    "Rob rob@gmail.com\n",
    "Ryan ryan@yahoo.com\n",
    "\"\"\"\n",
    "\n",
    "pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\.[A-Z]{2,4}'\n",
    "\n",
    "# re.IGNORECASE makes the regex case-insensitive\n",
    "regex = re.compile(pattern, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.177468Z",
     "start_time": "2021-01-27T16:21:42.163353Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using findall on the text produces a list of the email addresses\n",
    "regex.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.192624Z",
     "start_time": "2021-01-27T16:21:42.178477Z"
    }
   },
   "outputs": [],
   "source": [
    "# search returns a special match object for the first email address in the text\n",
    "regex.search(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.208093Z",
     "start_time": "2021-01-27T16:21:42.193885Z"
    }
   },
   "outputs": [],
   "source": [
    "# regex.match returns None, as it only will match if the pattern \n",
    "# occurs at the start of the string\n",
    "print(regex.match(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.224291Z",
     "start_time": "2021-01-27T16:21:42.209095Z"
    }
   },
   "outputs": [],
   "source": [
    "# sub will return a new string with occurrences of the pattern replaced by the\n",
    "# a new string\n",
    "print(regex.sub('REDACTED', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE**: Regular expression methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument                  | Description |\n",
    "| :---                  |    :----    |\n",
    "|findall| Return all non-overlapping matching patterns in a string as a list\n",
    "|finditer| Like findall, but returns an iterator\n",
    "|match| Match pattern at start of string and optionally segment pattern components into groups; if the pattern matches, returns a match object, and otherwise None\n",
    "|search| Scan string for match to pattern; returning a match object if so; unlike match, the match can be anywhere in the string as opposed to only at the beginning\n",
    "|split| Break string into pieces at each occurrence of pattern\n",
    "|sub, subn| Replace all (sub) or first n occurrences (subn) of pattern in string with replacement expression; use symbols \\1, \\2, ... to refer to match group elements in the replacement string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized String Functions in pandas\n",
    "- Cleaning up a messy dataset for analysis often requires a lot of string munging and regularization. \n",
    "- To complicate matters, a column containing strings will sometimes have missing data.\n",
    "- Series has array-oriented methods for string operations that skip NA values. These are accessed through Series’s **str** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.239298Z",
     "start_time": "2021-01-27T16:21:42.225298Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Series of strings with NA values\n",
    "data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com',\n",
    "        'Rob': 'rob@gmail.com', 'Wes': np.nan}\n",
    "data = pd.Series(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.254576Z",
     "start_time": "2021-01-27T16:21:42.240300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check whether each email address has 'gmail' in it with str.contains\n",
    "data.str.contains('gmail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.270299Z",
     "start_time": "2021-01-27T16:21:42.256263Z"
    }
   },
   "outputs": [],
   "source": [
    "# Regular expressions can be used, too, along with any re options like IGNORECASE\n",
    "data.str.findall('([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})', flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.286299Z",
     "start_time": "2021-01-27T16:21:42.272271Z"
    }
   },
   "outputs": [],
   "source": [
    "# Match pattern at start of string - returns a match object\n",
    "matches = data.str.match('([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})', flags=re.IGNORECASE)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-27T16:21:42.301648Z",
     "start_time": "2021-01-27T16:21:42.287908Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can slice strings using this syntax\n",
    "data.str[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE**: Partial listing of vectorized string methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Method                  | Description |\n",
    "| :---                  |    :----    |\n",
    "|cat| Concatenate strings element-wise with optional delimiter\n",
    "|contains| Return boolean array if each string contains pattern/regex\n",
    "|count| Count occurrences of pattern\n",
    "|extract| Use a regular expression with groups to extract one or more strings from a Series of strings; the result will be a DataFrame with one column per group\n",
    "|endswith| Equivalent to x.endswith(pattern) for each element\n",
    "|startswith| Equivalent to x.startswith(pattern) for each element\n",
    "|findall| Compute list of all occurrences of pattern/regex for each string\n",
    "|get| Index into each element (retrieve i-th element)\n",
    "|isalnum| Equivalent to built-in str.alnum\n",
    "|isalpha| Equivalent to built-in str.isalpha\n",
    "|isdecimal| Equivalent to built-in str.isdecimal\n",
    "|isdigit| Equivalent to built-in str.isdigit\n",
    "|islower| Equivalent to built-in str.islower\n",
    "|isnumeric| Equivalent to built-in str.isnumeric\n",
    "|isupper| Equivalent to built-in str.isupper\n",
    "|join| Join strings in each element of the Series with passed separator\n",
    "|len| Compute length of each string\n",
    "|lower, upper| Convert cases; equivalent to x.lower() or x.upper() for each element\n",
    "|match| Use re.match with the passed regular expression on each element, returning matched groups as list\n",
    "|pad| Add whitespace to left, right, or both sides of strings\n",
    "|center| Equivalent to pad(side='both')\n",
    "|repeat| Duplicate values (e.g., s.str.repeat(3) is equivalent to x * 3 for each string)\n",
    "|replace| Replace occurrences of pattern/regex with some other string\n",
    "|slice| Slice each string in the Series\n",
    "|split| Split strings on delimiter or regular expression\n",
    "|strip| Trim whitespace from both sides, including newlines\n",
    "|rstrip| Trim whitespace on right side\n",
    "|lstrip| Trim whitespace on left side"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "100px",
    "left": "146px",
    "top": "145px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
