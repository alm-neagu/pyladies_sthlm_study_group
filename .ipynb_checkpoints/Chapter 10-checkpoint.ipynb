{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHAPTER 10\n",
    "# Data Aggregation and Group Operations\n",
    "- Categorizing a dataset and applying a function to each group, whether an aggregation or transformation, is often a critical component of a data analysis workflow. \n",
    "- After loading, merging, and preparing a dataset, you may need to compute **group statistics** or possibly **pivot tables** for reporting or visualization purposes. \n",
    "- **pandas** provides a flexible ***groupby*** interface, enabling you to slice, dice, and summarize datasets in a natural way.\n",
    "- Aggregation of **time series** data, a special use case of groupby, is referred to as **resampling**.\n",
    "- This Chapter will focus on:\n",
    "\n",
    "    - Split a pandas object into pieces using one or more keys (in the form of functions, arrays, or DataFrame column names)\n",
    "    - Calculate group summary statistics, like count, mean, or standard deviation, or a user-defined function\n",
    "    - Apply within-group transformations or other manipulations, like normalization, linear regression, rank, or subset selection\n",
    "    - Compute pivot tables and cross-tabulations\n",
    "    - Perform quantile analysis and other statistical group analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy Mechanics\n",
    "- **group** operations can be described by the **split-apply-combine** method:\n",
    "    1. Data contained in a pandas object, whether a Series, DataFrame, is **split into groups** based on one or more **keys** that you provide. The splitting is performed on a particular axis of an object. For example, a DataFrame can be grouped on its rows (axis=0) or its columns (axis=1). \n",
    "    2. A **function is applied** to each group, producing a new value. \n",
    "    3. The results of all those function applications are **combined into a result object**. The form of the resulting object will usually depend on what’s being done to the data.\n",
    "- Each **grouping key** can take many forms, and the keys do not have to be all of the same type:\n",
    "    - A list or array of values that is the same length as the axis being grouped\n",
    "    - A value indicating a column name in a DataFrame\n",
    "    - A dict or Series giving a correspondence between the values on the axis being grouped and the group names\n",
    "    - A function to be invoked on the axis index or the individual labels in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.819437Z",
     "start_time": "2021-02-10T09:23:35.438149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.850047Z",
     "start_time": "2021-02-10T09:23:35.820086Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'key1': ['a', 'a', 'b', 'b', 'a'],\n",
    "    'key2': ['one', 'two', 'one', 'two', 'one'],\n",
    "    'data1': np.random.randn(5),\n",
    "    'data2': np.random.randn(5)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.865964Z",
     "start_time": "2021-02-10T09:23:35.851017Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a GroupBy object that has all of the information needed to apply some operation to\n",
    "# each of the groups\n",
    "grouped = df['data1'].groupby(df['key1'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.881971Z",
     "start_time": "2021-02-10T09:23:35.867991Z"
    }
   },
   "outputs": [],
   "source": [
    "# To compute group means we can call the GroupBy’s mean method\n",
    "grouped.mean()\n",
    "\n",
    "# The data (a Series) has been aggregated according to the group key,\n",
    "# producing a new Series that is now indexed by the unique values in the key1 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.897900Z",
     "start_time": "2021-02-10T09:23:35.882949Z"
    }
   },
   "outputs": [],
   "source": [
    "# If we group the data using two keys, the resulting Series now has a hierarchical index \n",
    "# consisting of the unique pairs of keys observed\n",
    "means = df['data1'].groupby([df['key1'], df['key2']]).mean()\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.913577Z",
     "start_time": "2021-02-10T09:23:35.899571Z"
    }
   },
   "outputs": [],
   "source": [
    "# Frequently the grouping information is found in the same DataFrame as the data you\n",
    "# want to work on. In that case, you can pass column names as the group keys\n",
    "df.groupby('key1').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the first case **df.groupby('key1').mean()** there is no **key2** column in the result. \n",
    "- Because **df['key2']** is not numeric data, it is said to be a nuisance column, which is therefore excluded from the result. \n",
    "- By default, all of the numeric columns are aggregated, though it is possible to filter down to a subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.928822Z",
     "start_time": "2021-02-10T09:23:35.914576Z"
    }
   },
   "outputs": [],
   "source": [
    "# A generally useful GroupBy method is size, which returns a Series containing group sizes\n",
    "df.groupby(['key1', 'key2']).size()\n",
    "\n",
    "# Take note that any missing values in a group key will be excluded from the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterating Over Groups\n",
    "- The GroupBy object supports **iteration**, generating a sequence of 2-tuples containing the group name along with the chunk of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.944347Z",
     "start_time": "2021-02-10T09:23:35.931809Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print name & group\n",
    "for name, group in df.groupby('key1'):\n",
    "    print(name)\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.959297Z",
     "start_time": "2021-02-10T09:23:35.945344Z"
    }
   },
   "outputs": [],
   "source": [
    "# In the case of multiple keys, the first element in the tuple will be a tuple of key values\n",
    "for (k1, k2), group in df.groupby(['key1', 'key2']):\n",
    "    print((k1, k2))\n",
    "    print(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a Column or Subset of Columns\n",
    "- Indexing a **GroupBy object** created from a DataFrame with a column name or array of column names has the effect of column subsetting for aggregation.\n",
    "- Especially for large datasets, it may be desirable to aggregate only a few columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.974905Z",
     "start_time": "2021-02-10T09:23:35.960295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute means for just the data2 column and get the result as a DataFrame\n",
    "df.groupby(['key1', 'key2'])[['data2']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:35.989703Z",
     "start_time": "2021-02-10T09:23:35.975896Z"
    }
   },
   "outputs": [],
   "source": [
    "# The object returned by this indexing operation is a grouped DataFrame if a list or\n",
    "# array is passed or a grouped Series if only a single column name is passed as a scalar\n",
    "s_grouped = df.groupby(['key1', 'key2'])['data2']\n",
    "s_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.004657Z",
     "start_time": "2021-02-10T09:23:35.990693Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the mean operation\n",
    "s_grouped.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping with Dicts and Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.019376Z",
     "start_time": "2021-02-10T09:23:36.005651Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create example DataFrame\n",
    "people = pd.DataFrame(np.random.randn(5, 5),\n",
    "                      columns=['a', 'b', 'c', 'd', 'e'],\n",
    "                      index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\n",
    "\n",
    "people.iloc[2:3, [1, 2]] = np.nan # Add a few NA values\n",
    "people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.034420Z",
     "start_time": "2021-02-10T09:23:36.021369Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping for columns\n",
    "mapping = {'a': 'red', 'b': 'red', 'c': 'blue','d': 'blue', 'e': 'red', 'f' : 'orange'}\n",
    "\n",
    "# Unused groups are ok like 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.050375Z",
     "start_time": "2021-02-10T09:23:36.035417Z"
    }
   },
   "outputs": [],
   "source": [
    "# To group the columns we can just pass the dict to the groupby function\n",
    "by_column = people.groupby(mapping, axis=1).sum()\n",
    "by_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping with Functions\n",
    "- Using Python functions is a more generic way of defining a group mapping compared with a dict or Series. \n",
    "- Any function passed as a group key will be called once per index value, with the return values being used as the group names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.065838Z",
     "start_time": "2021-02-10T09:23:36.051630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group by the length of the names\n",
    "people.groupby(len).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by Index Levels\n",
    "- For hierarchically indexed datasets you can aggregate using one of the levels of an axis index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.081204Z",
     "start_time": "2021-02-10T09:23:36.067829Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create example DataFrame\n",
    "columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'], [1, 3, 5, 1, 3]],\n",
    "                                    names=['cty', 'tenor'])\n",
    "hier_df = pd.DataFrame(np.random.randn(4, 5), columns=columns)\n",
    "hier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.096459Z",
     "start_time": "2021-02-10T09:23:36.083295Z"
    }
   },
   "outputs": [],
   "source": [
    "# To group by level, pass the level number or name using the level keyword\n",
    "hier_df.groupby(level='cty', axis=1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Aggregation\n",
    "- Aggregations refer to any data transformation that produces scalar values from arrays.\n",
    "- Many common aggregations, such as mean, count, min and sum have optimized implementations.\n",
    "- You can use aggregations of your own devising and additionally call any method that is also defined on the grouped object.\n",
    "- Custom aggregation functions are generally much slower than the optimized functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE**: Optimized groupby methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument                  | Description |\n",
    "| :---                  |    :----    |\n",
    "|count| Number of non-NA values in the group\n",
    "|sum| Sum of non-NA values\n",
    "|mean| Mean of non-NA values\n",
    "|median| Arithmetic median of non-NA values\n",
    "|std, var| Unbiased (n – 1 denominator) standard deviation and variance\n",
    "|min, max| Minimum and maximum of non-NA values\n",
    "|prod| Product of non-NA values\n",
    "|first, last| First and last non-NA values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.111209Z",
     "start_time": "2021-02-10T09:23:36.098251Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our example DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.126799Z",
     "start_time": "2021-02-10T09:23:36.112211Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a GroupBy object\n",
    "grouped = df.groupby('key1')\n",
    "\n",
    "# Apply the quantile function to the GroupBy object\n",
    "grouped['data1'].quantile(0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To use your own aggregation functions, pass any function that aggregates an array to the **aggregate or agg method**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.142745Z",
     "start_time": "2021-02-10T09:23:36.128792Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function that aggregates\n",
    "def peak_to_peak(arr):\n",
    "    return arr.max() - arr.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.158284Z",
     "start_time": "2021-02-10T09:23:36.143751Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the function the the GroupBy object\n",
    "grouped.agg(peak_to_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.206121Z",
     "start_time": "2021-02-10T09:23:36.161273Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some methods like describe also work, even though they are not aggregations\n",
    "grouped.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column-Wise and Multiple Function Application\n",
    "- **Aggregating** a Series or all of the columns of a DataFrame is a matter of using **aggregate** with the desired function or calling a method like **mean** or **std**. \n",
    "- However, you may want to aggregate using a different function depending on the column, or multiple functions at once.\n",
    "- For **descriptive statistics** like those in Table *Optimized groupby methods*, you can pass the name of the function as a string to the **aggregate** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.222068Z",
     "start_time": "2021-02-10T09:23:36.208114Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load tipping dataset\n",
    "tips = pd.read_csv('examples/tips.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.253642Z",
     "start_time": "2021-02-10T09:23:36.224061Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add tip percentage of total bill\n",
    "tips['tip_pct'] = tips['tip'] / tips['total_bill']\n",
    "tips[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:23:36.269581Z",
     "start_time": "2021-02-10T09:23:36.254631Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group the tips by day and smoker\n",
    "grouped = tips.groupby(['day', 'smoker'])\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:26:01.286293Z",
     "start_time": "2021-02-10T09:26:01.274543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the mean tip percentage by day & smoker\n",
    "grouped_pct = grouped['tip_pct']\n",
    "grouped_pct.agg('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:29:19.649471Z",
     "start_time": "2021-02-10T09:29:19.637741Z"
    }
   },
   "outputs": [],
   "source": [
    "# If you pass a list of functions or function names instead, you get back a DataFrame\n",
    "# with column names taken from the functions\n",
    "grouped_pct.agg(['mean', 'std', peak_to_peak])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:30:05.624874Z",
     "start_time": "2021-02-10T09:30:05.602748Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can pass a a list of (name, function) tuples to define the name of the columns\n",
    "grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:36:07.958184Z",
     "start_time": "2021-02-10T09:36:07.937529Z"
    }
   },
   "outputs": [],
   "source": [
    "# With a DataFrame you have more options, as you can specify a list of functions to\n",
    "# apply to all of the columns or different functions per column\n",
    "\n",
    "# Define a list of functions\n",
    "functions = ['count', 'mean', 'max']\n",
    "\n",
    "# Apply these fuction to columns 'tip_pct' & 'total_bill'\n",
    "result = grouped[['tip_pct', 'total_bill']].agg(functions)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:40:18.402630Z",
     "start_time": "2021-02-10T09:40:18.385097Z"
    }
   },
   "outputs": [],
   "source": [
    "# To apply different functions to one or more of the columns pass a dict to agg \n",
    "# that contains a mapping of column names to any of the functions\n",
    "\n",
    "grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'], 'size' : 'sum'})\n",
    "\n",
    "# A DataFrame will have hierarchical columns only if multiple functions are applied to\n",
    "# at least one column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning Aggregated Data Without Row Indexes\n",
    "- In all of the examples up until now, the aggregated data comes back with an **index**, potentially hierarchical, composed from the unique group key combinations. \n",
    "- Since this isn’t always desirable, you can disable this behavior in most cases by passing **as_index=False** to groupby.\n",
    "- It’s always possible to obtain the result in this format by calling **reset_index** on the result. Using the as_index=False method avoids some unnecessary computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:46:56.833345Z",
     "start_time": "2021-02-10T09:46:56.801687Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mean values by day & smoker with no index\n",
    "tips.groupby(['day', 'smoker'], as_index=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply: General split-apply-combine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Returning to the tipping dataset from before, suppose you wanted to select the top five tip_pct values by group. \n",
    "- First, write a function that selects the rows with the largest values in a particular column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:52:30.848210Z",
     "start_time": "2021-02-10T09:52:30.829670Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to select top 5 tip_pct\n",
    "def top(df, n=5, column='tip_pct'):\n",
    "    return df.sort_values(by=column)[-n:]\n",
    "\n",
    "# Use the fucntion on the tips DataFrame\n",
    "top(tips, n=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T09:57:58.638246Z",
     "start_time": "2021-02-10T09:57:58.623416Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group by smoker and call apply with the top function\n",
    "tips.groupby('smoker').apply(top, n=3, column='total_bill')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The top function is called on each **row group** from the DataFrame, and then the results are glued together using **pandas.concat**, labeling the pieces with the group names. \n",
    "- The result therefore has a **hierarchical index** whose inner level contains index values from the original DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suppressing the Group Keys\n",
    "- In the preceding examples, you see that the resulting object has a **hierarchical index** formed from the group keys along with the indexes of each piece of the original object. \n",
    "- You can disable this by passing **group_keys=False** to groupby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:01:00.067965Z",
     "start_time": "2021-02-10T10:01:00.052361Z"
    }
   },
   "outputs": [],
   "source": [
    "# Suppress the group keys\n",
    "tips.groupby('smoker', group_keys=False).apply(top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantile and Bucket Analysis\n",
    "- **pandas** has some tools, in particular **cut** and **qcut**, for slicing data up into buckets with bins of your choosing or by sample quantiles (Chapter 8).\n",
    "- Combining these functions with groupby makes it convenient to perform bucket or quantile analysis on a dataset.\n",
    "- The **Categorical object** returned by **cut** can be passed directly to **groupby**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:06:00.192045Z",
     "start_time": "2021-02-10T10:06:00.170371Z"
    }
   },
   "outputs": [],
   "source": [
    "# A simple random dataset\n",
    "frame = pd.DataFrame({'data1': np.random.randn(1000),\n",
    "                      'data2': np.random.randn(1000)})\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:06:34.176451Z",
     "start_time": "2021-02-10T10:06:34.148099Z"
    }
   },
   "outputs": [],
   "source": [
    "# Equal-length bucket categorization\n",
    "quartiles = pd.cut(frame.data1, 4)\n",
    "quartiles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:08:26.680704Z",
     "start_time": "2021-02-10T10:08:26.664981Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute a set of statistics with the function get_stats\n",
    "def get_stats(group):\n",
    "    return {'min': group.min(), 'max': group.max(), 'count': group.count(), 'mean': group.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:09:42.398353Z",
     "start_time": "2021-02-10T10:09:42.382536Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a GroupBy oject using the quartiles defined above\n",
    "grouped = frame.data2.groupby(quartiles)\n",
    "grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:10:21.323067Z",
     "start_time": "2021-02-10T10:10:21.305860Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply the fucntion get_stats\n",
    "grouped.apply(get_stats).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:11:26.274934Z",
     "start_time": "2021-02-10T10:11:26.254582Z"
    }
   },
   "outputs": [],
   "source": [
    "# To compute equal-size buckets based on sample quantiles, use qcut\n",
    "grouping = pd.qcut(frame.data1, 10, labels=False)\n",
    "grouped = frame.data2.groupby(grouping)\n",
    "grouped.apply(get_stats).unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Filling Missing Values with Group-Specific Values\n",
    "- When cleaning up missing data, in some cases you will replace data observations using **dropna**, but in others you may want to impute (fill in) the null (NA) values using a fixed value or some value derived from the data. \n",
    "- **fillna** is the right tool to use and you can for example fill in NA values with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:22:00.274955Z",
     "start_time": "2021-02-10T10:22:00.257604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create example Series\n",
    "s = pd.Series(np.random.randn(6))\n",
    "\n",
    "# Ass some NA values\n",
    "s[::2] = np.nan\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:22:16.352471Z",
     "start_time": "2021-02-10T10:22:16.338504Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill NA values with the mean\n",
    "s.fillna(s.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:24:56.985174Z",
     "start_time": "2021-02-10T10:24:56.967580Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of states\n",
    "states = ['Ohio', 'New York', 'Vermont', 'Florida', 'Oregon', 'Nevada', 'California', 'Idaho']\n",
    "\n",
    "# List of cardinal directions\n",
    "group_key = ['East'] * 4 + ['West'] * 4\n",
    "\n",
    "# Convert to series\n",
    "data = pd.Series(np.random.randn(8), index=states)\n",
    "\n",
    "# Add some missing values\n",
    "data[['Vermont', 'Nevada', 'Idaho']] = np.nan\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:26:10.795179Z",
     "start_time": "2021-02-10T10:26:10.776738Z"
    }
   },
   "outputs": [],
   "source": [
    "# We can fill the NA values using the group means\n",
    "\n",
    "fill_mean = lambda g: g.fillna(g.mean())\n",
    "data.groupby(group_key).apply(fill_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T10:27:32.136896Z",
     "start_time": "2021-02-10T10:27:32.121947Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in with predefined values that vary by group\n",
    "\n",
    "fill_values = {'East': 0.5, 'West': -1}\n",
    "fill_func = lambda g: g.fillna(fill_values[g.name])\n",
    "data.groupby(group_key).apply(fill_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Group Weighted Average and Correlation\n",
    "- Under the **split-apply-combine** paradigm of **groupby**, operations between columns in a DataFrame or two Series, such as a group weighted average, are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:39:44.299642Z",
     "start_time": "2021-02-10T14:39:44.279229Z"
    }
   },
   "outputs": [],
   "source": [
    "# Example dataset containing group keys, values, and some weights\n",
    "df = pd.DataFrame({'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'],\n",
    "                   'data': np.random.randn(8),\n",
    "                   'weights': np.random.rand(8)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:40:56.259442Z",
     "start_time": "2021-02-10T14:40:56.241620Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group weighted average by category\n",
    "grouped = df.groupby('category') \n",
    "get_wavg = lambda g: np.average(g['data'], weights=g['weights'])\n",
    "grouped.apply(get_wavg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:42:03.079933Z",
     "start_time": "2021-02-10T14:42:03.046337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finance containing end-of-day prices for a few stocks and the S&P 500 index from Yahoo!\n",
    "close_px = pd.read_csv('examples/stock_px_2.csv', parse_dates=True, index_col=0)\n",
    "close_px.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:42:44.339739Z",
     "start_time": "2021-02-10T14:42:44.332761Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check the dataset\n",
    "close_px[-4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:44:11.513509Z",
     "start_time": "2021-02-10T14:44:11.487678Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function that computes the pairwise correlation of each column with the 'SPX' column\n",
    "spx_corr = lambda x: x.corrwith(x['SPX'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:48:12.725972Z",
     "start_time": "2021-02-10T14:48:12.707252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Percent change on close_px using pct_change\n",
    "rets = close_px.pct_change().dropna()\n",
    "rets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:46:37.092776Z",
     "start_time": "2021-02-10T14:46:37.030400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group these percent changes by year\n",
    "get_year = lambda x: x.year\n",
    "by_year = rets.groupby(get_year)\n",
    "by_year.apply(spx_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:47:34.415206Z",
     "start_time": "2021-02-10T14:47:34.395261Z"
    }
   },
   "outputs": [],
   "source": [
    "# You could also compute inter-column correlations\n",
    "# Here we compute the annual correlation between Apple and Microsoft\n",
    "by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Group-Wise Linear Regression\n",
    "- You can use **groupby** to perform more complex group-wise statistical analysis, as long as the function returns a pandas object or scalar value. \n",
    "- For example, you can define the following regress function (using the statsmodels econometrics library), which executes an ordinary least squares (OLS) regression on each chunk of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:51:55.002736Z",
     "start_time": "2021-02-10T14:51:54.032798Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the regress function\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def regress(data, yvar, xvars):\n",
    "    Y = data[yvar]\n",
    "    X = data[xvars]\n",
    "    X['intercept'] = 1.\n",
    "    result = sm.OLS(Y, X).fit()\n",
    "    return result.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:52:30.634489Z",
     "start_time": "2021-02-10T14:52:30.582927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run a yearly linear regression for AAPL on SPX\n",
    "by_year.apply(regress, 'AAPL', ['SPX'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables and Cross-Tabulation\n",
    "- A **pivot table** is a data summarization tool frequently found in spreadsheet programs and other data analysis software. \n",
    "- It aggregates a table of data by one or more keys, arranging the data in a rectangle with some of the group keys along the rows and some along the columns.\n",
    "- DataFrame has a **pivot_table** method, and there is also a top-level **pandas.pivot_table function**. \n",
    "- In addition to providing a convenience interface to groupby, pivot_table can add **partial totals**, also known as margins.\n",
    "- The default pivot_table aggregation type is **mean**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:56:29.230546Z",
     "start_time": "2021-02-10T14:56:29.213519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute a table of group means by smoker & day\n",
    "tips.pivot_table(index=['day', 'smoker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:57:47.086678Z",
     "start_time": "2021-02-10T14:57:47.056506Z"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate only tip_pct and size and group by time\n",
    "# With smoker in the table columns and day in the rows\n",
    "tips.pivot_table(['tip_pct', 'size'], index=['time', 'day'], columns='smoker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T14:58:33.239814Z",
     "start_time": "2021-02-10T14:58:33.199707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Include partial totals\n",
    "tips.pivot_table(['tip_pct', 'size'], index=['time', 'day'], columns='smoker', margins=True)\n",
    "\n",
    "# All values are means without taking into account any grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:00:21.533221Z",
     "start_time": "2021-02-10T15:00:21.499624Z"
    }
   },
   "outputs": [],
   "source": [
    "# To use a different aggregation function, pass it to aggfunc\n",
    "tips.pivot_table('tip_pct', index=['time', 'smoker'], columns='day', aggfunc=len, margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TABLE**: pivot_table options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Argument                  | Description |\n",
    "| :---                  |    :----    |\n",
    "|values| Column name or names to aggregate; by default aggregates all numeric columns\n",
    "|index| Column names or other group keys to group on the rows of the resulting pivot table\n",
    "|columns| Column names or other group keys to group on the columns of the resulting pivot table\n",
    "|aggfunc| Aggregation function or list of functions ('mean' by default); can be any function valid in a groupby context\n",
    "|fill_value| Replace missing values in result table\n",
    "|dropna| If True, do not include columns whose entries are all NA\n",
    "|margins| Add row/column subtotals and grand total (False by default)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Tabulations: Crosstab\n",
    "- A **cross-tabulation** (or crosstab for short) is a special case of a pivot table that computes group frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:12:00.134715Z",
     "start_time": "2021-02-10T15:12:00.117360Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "data = pd.read_csv('datasets/bitly_usagov/sample_nationality.txt')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-10T15:12:26.534758Z",
     "start_time": "2021-02-10T15:12:26.501621Z"
    }
   },
   "outputs": [],
   "source": [
    "# Summarize this data by nationality and handedness\n",
    "pd.crosstab(data.Nationality, data.Handedness, margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "47.3125px",
    "left": "1034px",
    "top": "110px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
